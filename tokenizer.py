from api_tokenizer import APITokenizer
from transformers import AutoTokenizer
import copy
import numpy as np

class Tokenizer(APITokenizer):
	def __init__(self, model_name:str=None):
		"""Initializes a HuggingFace tokenizer with a specific model or a default one."""
		super().__init__()

		if model_name is None:
			model_name = "togethercomputer/LLaMA-2-7B-32K"
		self.model_name = model_name

		try:
			self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
		except Exception as e:
			print(f"Failed to load tokenizer for model {self.model_name}: {e}")
			raise e

	def encode(self, text):
		"""Encodes a given text into tokens."""
		try:
			return self.tokenizer.encode(text)
		except Exception as e:
			print(f"Failed to encode text: {e}")
			raise e

	def decode(self, token_ids):
		"""Decodes token ids back to text."""
		try:
			return self.tokenizer.decode(token_ids)
		except Exception as e:
			print(f"Failed to decode tokens: {e}")
			raise e

	def __call__(self, text):
		"""Processes input text or a list of texts using the encode method."""
		if isinstance(text, str):
			return self.encode(text)
		elif isinstance(text, list):
			return [self.encode(t) for t in text]
		else:
			raise TypeError("Input must be a string or a list of strings.")

class Tokenizer(APITokenizer):
	def __init__(
			self,
			huggingface_name=None,
			name=None,
			add_eos_token=False,
			add_bos_token=False,
			bos_token="<s>",
			eos_token="</s>",
			pad_token="<pad>",
			sep_token="<sep>",
			bos_token_id=31494,
			# bos_token_id=1,
			eos_token_id=2,
			pad_token_id=0,
			sep_token_id=32001-2,
			vocab_size=32000,
			**kwargs,):
		super().__init__()
		
		bos_token_id=31494
		sep_token_id=31999
		vocab_size=32000
		eos_token_id = 2

		self.infilling_sentinel_token_id_start = 31900
		self.infilling_sentinel_token_id_end = 32000

		self.autoencoding_sentinel_token_id_start = 31900
		self.autoencoding_sentinel_token_id_end = 32000


		# Makes chat tag easier read during debugging, particularly with packed sequences
		self.special_matches = [
			("</s>", "â–ˆ",),
		]

		if huggingface_name is None:
			huggingface_name = "togethercomputer/LLaMA-2-7B-32K"
		self.huggingface_name = huggingface_name

		
		vocabulary = AutoTokenizer.from_pretrained(self.huggingface_name)
		
		self.name = name
		self.tokenizer = vocabulary
		self.vocab_size = vocab_size
		self.bos_token_id = bos_token_id
		self.eos_token_id = eos_token_id
		self.pad_token_id = pad_token_id
		self.sep_token_id = sep_token_id
		self.bos_token = bos_token
		self.eos_token = eos_token
		self.pad_token = pad_token
		self.sep_token = sep_token
		self.add_eos_token = add_eos_token
		self.add_bos_token = add_bos_token

		self.pad_id = pad_token_id
		self.bos_id = bos_token_id
		self.eos_id = eos_token_id
		self.sep_id = sep_token_id

		trs = [self.tokenizer.decode([idx]).strip()[0] for idx in range(31900, 32000)]

		# Replace unused tokens with emojis for easier debugging
		trs = "ä¹¦æ„ç±³è¿æ“è£…å’ŒãåÌŒä»®å‘˜æ˜­à´¶å…´å®¢åˆ à¶¸à·€áƒÄ‹à´·á€áµ‰å±…íƒ€ğ“à¤¥ç¾Ë‡ì¢…åŠ©å”ç€¬á“å¾®ï¼‘Ä ã»èˆë‚´ì¤‘Ä’å¯¼æ•ˆë°©á¸æ·±æ¢…æ–™ì›”æ¯æ´²íšŒèŒ¶è´¥à´á»ƒãƒ¨äº›åŒå˜‰ëª¨ë°”à¸©é€²ìŒà¸ä¸æ•…è¨ˆé êµì¬å€™æˆ¿ëª…ä¸¤áƒ¤æ‰í•©æ­¢ç•ªÉ¯å¥‡æ€ªè”ì—­æ³°ë°±á½€ã’ã¹è¾¹è¿˜é»ƒì™•æ”¶å¼˜ç»™ä¹¦æ„ç±³è¿æ“è£…å’ŒãåÌŒä»®å‘˜æ˜­à´¶å…´å®¢åˆ à¶¸à·€áƒÄ‹à´·á€áµ‰å±…íƒ€ğ“à¤¥ç¾Ë‡ì¢…åŠ©å”ç€¬á“å¾®ï¼‘Ä ã»èˆë‚´ì¤‘Ä’å¯¼æ•ˆë°©á¸æ·±æ¢…æ–™ì›”æ¯æ´²íšŒèŒ¶è´¥à´á»ƒãƒ¨äº›åŒå˜‰ëª¨ë°”à¸©é€²ìŒà¸ä¸"
		decode_emojis = "ğŸŒğŸŒ‹ğŸ—»ğŸ ğŸ¡â›ªï¸ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ¯ğŸ°ğŸ’’ğŸ—¼ğŸ—½ğŸ—¾â›²ï¸â›ºï¸ğŸŒğŸŒƒğŸŒ„ğŸŒ…ğŸŒ†ğŸŒ‡ğŸŒ‰â™¨ï¸ğŸŒŒğŸ ğŸ¡ğŸ¢ğŸ’ˆğŸªğŸ­ğŸ¨ğŸ°ğŸš‚ğŸšƒğŸš„ğŸš…ğŸš†ğŸš‡ğŸšˆğŸš‰ğŸšŠğŸšğŸšğŸš‹ğŸšŒğŸšğŸšğŸšğŸšğŸš‘ğŸš’ğŸš“ğŸš”ğŸš•ğŸš–ğŸš—ğŸš˜ğŸš™ğŸššğŸš›ğŸšœğŸš²â›½ï¸ğŸš¨ğŸš¥ğŸš¦ğŸš§âš“ï¸â›µï¸ğŸš£ğŸš¤ğŸš¢âœˆï¸ğŸ’ºğŸšğŸšŸğŸš ğŸš¡ğŸš€ğŸšªâ³âŒšï¸â°ğŸ•“ğŸŒ‘ğŸŒ“ğŸŒ•ğŸŒšğŸŒğŸŒğŸŒ â˜ï¸â›…ï¸ğŸŒ€ğŸŒˆğŸŒ‚â˜”ï¸âš¡ï¸â„ï¸â›„ï¸ğŸ”¥ğŸ’§ğŸŒŠğŸƒğŸ„ğŸ†ğŸ‡âœ¨ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸğŸğŸğŸ‘ğŸ€ğŸğŸ«âš½ï¸âš¾ï¸ğŸ€ğŸˆğŸ‰ğŸ¾ğŸ±ğŸ³â›³ï¸ğŸ£ğŸ½ğŸ¿ğŸ‚ğŸ„ğŸ‡ğŸŠğŸš´ğŸšµğŸ†ğŸ¯ğŸ®ğŸ²â¬›ï¸â¬œï¸ğŸ”¶ğŸ”·ğŸ”¸ğŸ”¹ğŸ”ºğŸ”»ğŸ’ ğŸ”˜ğŸ”²ğŸ”³âšªï¸âš«ï¸ğŸ”´ğŸ”µğŸ‡¨ğŸ‡³ğŸ‡©ğŸ‡ªğŸ‡ªğŸ‡¸ğŸ‡«ğŸ‡·ğŸ‡¬ğŸ‡§ğŸ‡®ğŸ‡¹ğŸ‡¯ğŸ‡µğŸ‡°ğŸ‡·ğŸ‡·ğŸ‡ºğŸ‡ºğŸ‡¸"
		decode_emojis = "ğŸŒğŸŒ‹ğŸ—»ğŸ ğŸ®â›ªï¸ğŸ¢ğŸ£ğŸ¤ğŸ¥ğŸ¦ğŸ¨ğŸ©ğŸªğŸ«ğŸ¬ğŸ­ğŸ¯ğŸ°ğŸ’’ğŸ—¼ğŸ—½ğŸ—¾â›²ï¸â›ºï¸ğŸŒğŸŒƒğŸŒ„ğŸŒ…ğŸŒ†ğŸŒ‡ğŸŒ‰â™¨ï¸ğŸŒŒğŸ ğŸ¡ğŸ¢ğŸ’ˆğŸªğŸ­ğŸ¨ğŸ°ğŸš‚ğŸšƒğŸš„ğŸ”¶ğŸš†ğŸš‡ğŸšˆğŸš‰ğŸšŠğŸšğŸšğŸš‹ğŸšŒğŸšğŸ”·ğŸšğŸšğŸš‘ğŸš’ğŸš“ğŸš”ğŸš•ğŸš–ğŸš—ğŸš˜ğŸš™ğŸššğŸš›ğŸšœğŸš²â›½ï¸ğŸš¨ğŸš¥ğŸš¦ğŸš§âš“ï¸â›µï¸ğŸš£ğŸš¤ğŸš¢âœˆï¸ğŸ’ºğŸšğŸšŸğŸš ğŸš¡ğŸš€ğŸšªâ³âŒšï¸â°ğŸ•“ğŸŒ‘ğŸŒ“ğŸŒ•ğŸŒšğŸŒğŸŒğŸŒ â˜ï¸â›…ï¸ğŸŒ€ğŸŒˆğŸŒ‚â˜”ï¸âš¡ï¸â„ï¸â›„ï¸ğŸ”¥ğŸ’§ğŸŒŠğŸƒğŸ„ğŸ†ğŸ‡âœ¨ğŸˆğŸ‰ğŸŠğŸ‹ğŸŒğŸğŸğŸğŸğŸ‘ğŸ€ğŸğŸ«âš½ï¸âš¾ï¸ğŸ€ğŸˆğŸ‰ğŸ¾ğŸ±ğŸ³â›³ï¸ğŸ£ğŸ½ğŸ¿ğŸ‚ğŸ„ğŸ‡ğŸŠğŸš´ğŸšµğŸ†ğŸ¯"
		
		self.special_matches_decoding = []
		for tr, emoji in zip(trs, decode_emojis):
			self.special_matches_decoding.append((tr, emoji))


		self.special_matches_decoding = [
			[0,'ä¹¦','ğŸŒ',],
			[1,'æ„','ğŸŒ‹',],
			[2,'ç±³','ğŸ—»',],
			[3,'è¿','ğŸ ',],
			[4,'æ“','ğŸ®',],
			[5,'è£…','â›ª',],
			[6,'å’Œ','ğŸ‡¬ğŸ‡§',],
			[7,'ã','ğŸ¢',],
			[8,'å','ğŸ£',],
			[9,'åÌŒ '[1:],'ğŸ¤',],
			[10,'ä»®','ğŸ¥',],
			[11,'å‘˜','ğŸ¦',],
			[12,'æ˜­','ğŸ¨',],
			[13,'à´¶','ğŸ©',],
			[14,'å…´','ğŸª',],
			[15,'å®¢','ğŸ«',],
			[16,'åˆ ','ğŸ¬',],
			[17,'à¶¸','ğŸ­',],
			[18,'à·€','ğŸ¯',],
			[19,'áƒ','ğŸ°',],
			[20,'Ä‹','ğŸ’’',],
			[21,'à´·','ğŸ—¼',],
			[22,'á€','ğŸ—½',],
			[23,'áµ‰','ğŸ—¾',],
			[24,'å±…','â›²',],
			[25,'íƒ€','ğŸ‡®ğŸ‡¹',],
			[26,'ğ“','â›º',],
			[27,'à¤¥','ğŸ‘½',],
			[28,'ç¾','ğŸŒ',],
			[29,'Ë‡','ğŸŒƒ',],
			[30,'ì¢…','ğŸŒ„',],
			[31,'åŠ©','ğŸŒ…',],
			[32,'å”','ğŸŒ†',],
			[33,'ç€¬','ğŸŒ‡',],
			[34,'á“','ğŸŒ‰',],
			[35,'å¾®','â™¨',],
			[36,'ï¼‘','ğŸ‡¯ğŸ‡µ',],
			[37,'Ä ','ğŸŒŒ',],
			[38,'ã»','ğŸ ',],
			[39,'èˆ','ğŸ¡',],
			[40,'ë‚´','ğŸ¢',],
			[41,'ì¤‘','ğŸ’ˆ',],
			[42,'Ä’','ğŸª',],
			[43,'å¯¼','ğŸ­',],
			[44,'æ•ˆ','ğŸ¨',],
			[45,'ë°©','ğŸ°',],
			[46,'á¸','ğŸš‚',],
			[47,'æ·±','ğŸšƒ',],
			[48,'æ¢…','ğŸš„',],
			[49,'æ–™','ğŸ”¶',],
			[50,'ì›”','ğŸš†',],
			[51,'æ¯','ğŸš‡',],
			[52,'æ´²','ğŸšˆ',],
			[53,'íšŒ','ğŸš‰',],
			[54,'èŒ¶','ğŸšŠ',],
			[55,'è´¥','ğŸš',],
			[56,'à´','ğŸš',],
			[57,'á»ƒ','ğŸš‹',],
			[58,'ãƒ¨','ğŸšŒ',],
			[59,'äº›','ğŸš',],
			[60,'åŒ','ğŸ”·',],
			[61,'å˜‰','ğŸš',],
			[62,'ëª¨','ğŸš',],
			[63,'ë°”','ğŸš‘',],
			[64,'à¸©','ğŸš’',],
			[65,'é€²','ğŸš“',],
			[66,'ìŒ','ğŸš”',],
			[67,'à¸','ğŸš•',],
			[68,'ä¸','ğŸš–',],
			[69,'æ•…','ğŸš—',],
			[70,'è¨ˆ','ğŸš˜',],
			[71,'é ','ğŸš™',],
			[72,'êµ','ğŸšš',],
			[73,'ì¬','ğŸš›',],
			[74,'å€™','ğŸšœ',],
			[75,'æˆ¿','ğŸš²',],
			[76,'ëª…','â›½',],
			[77,'ä¸¤','ğŸƒ',],
			[78,'áƒ¤','ğŸš¨',],
			[79,'æ‰','ğŸš¥',],
			[80,'í•©','ğŸš¦',],
			[81,'æ­¢','ğŸš§',],
			[82,'ç•ª','âš“',],
			[83,'É¯','ğŸ‡ºğŸ‡¸',],
			[84,'å¥‡','â›µ',],
			[85,'æ€ª','ğŸ‡¬ğŸ‡ª',],
			[86,'è”','ğŸš£',],
			[87,'ì—­','ğŸš¤',],
			[88,'æ³°','ğŸš¢',],
			[89,'ë°±','âœˆ',],
			[90,'á½€','ğŸ§Ÿ',],
			[91,'ã’','ğŸ’º',],
			[92,'ã¹','ğŸš',],
			[93,'è¾¹','ğŸšŸ',],
			[94,'è¿˜','ğŸš ',],
			[95,'é»ƒ','ğŸš¡',],
			[96,'ì™•','ğŸš€',],
			[97,'æ”¶','ğŸšª',],
			[98,'å¼˜','â³',],
			[99,'ç»™','âŒš',],
			[100,'ä¹¦','ğŸ‡¦ğŸ‡¬',],
			[101,'æ„','â°',],
			[102,'ç±³','ğŸ•“',],
			[103,'è¿','ğŸŒ‘',],
			[104,'æ“','ğŸŒ“',],
			[105,'è£…','ğŸŒ•',],
			[106,'å’Œ','ğŸŒš',],
			[107,'ã','ğŸŒ',],
			[108,'å','ğŸŒ',],
			[109,'gÌŒ','ğŸ‡ºğŸ‡¦',],
			[110,'ä»®','â˜',],
			[111,'å‘˜','ğŸª–',],
			[112,'æ˜­','â›…',],
			[113,'à´¶','ğŸ©¸',],
			[114,'å…´','ğŸŒ€',],
			[115,'å®¢','ğŸŒˆ',],
			[116,'åˆ ','ğŸŒ‚',],
			[117,'à¶¸','â˜”',],
			[118,'à·€','ğŸŒ»',],
			[119,'áƒ','âš¡',],
			[120,'Ä‹','ğŸª¦',],
			[121,'à´·','â„',],
			[122,'á€','ğŸ¤',],
			[123,'áµ‰','â›„',],
			[124,'å±…','ğŸ–ï¸',],
			[125,'íƒ€','ğŸ”¥',],
			[126,'ğ“','ğŸ’§',],
			[127,'à¤¥','ğŸŒŠ',],
			[128,'ç¾','ğŸƒ',],
			[129,'Ë‡','ğŸ„',],
			[130,'ì¢…','ğŸ†',],
			[131,'åŠ©','ğŸ‡',],
			[132,'å”','âœ¨',],
			[133,'ç€¬','ğŸˆ',],
			[134,'á“','ğŸ‰',],
			[135,'å¾®','ğŸŠ',],
			[136,'ï¼‘','ğŸ‹',],
			[137,'Ä ','ğŸŒ',],
			[138,'ã»','ğŸ',],
			[139,'èˆ','ğŸ',],
			[140,'ë‚´','ğŸ',],
			[141,'ì¤‘','ğŸ',],
			[142,'Ä’','ğŸ‘',],
			[143,'å¯¼','ğŸ€',],
			[144,'æ•ˆ','ğŸ',],
			[145,'ë°©','ğŸ«',],
			[146,'á¸','âš½',],
			[147,'æ·±','ğŸ›¸',],
			[148,'æ¢…','âš¾',],
			[149,'æ–™','ğŸ§ ',],
			[150,'ì›”','ğŸ€',],
			[151,'æ¯','ğŸˆ',],
			[152,'æ´²','ğŸ‰',],
			[153,'íšŒ','ğŸ¾',],
			[154,'èŒ¶','ğŸ±',],
			[155,'è´¥','ğŸ³',],
			[156,'à´','â›³',],
			[157,'á»ƒ','ğŸ’™',],
			[158,'ãƒ¨','ğŸ£',],
			[159,'äº›','ğŸ½',],
			[160,'åŒ','ğŸ¿',],
			[161,'å˜‰','ğŸ‚',],
			[162,'ëª¨','ğŸ„',],
			[163,'ë°”','ğŸ‡',],
			[164,'à¸©','ğŸŠ',],
			[165,'é€²','ğŸš´',],
			[166,'ìŒ','ğŸšµ',],
			[167,'à¸','ğŸ†',],
			[168,'ä¸','ğŸ¯',],

			[169,'Ò¡','ğŸ‡¸ğŸ‡ª',],
			# [170,''à¾±','ğŸ‡¨ğŸ‡¦',],
		]
	
	def orca_encode(self, x):
		if isinstance(x, list):
			return [self.orca_encode(e) for e in x]
		for special_matches_idx, (query, value) in enumerate(self.special_matches):
			x = x.replace(query, value)
		return self.encode(x)
	
	# ## allow for any other function to go through self.vocab
	# def __getattr__(self, name):
	# 	return getattr(self.vocabulary, name)
	
	def __getattr__(self, name):
		if hasattr(self.vocabulary, name):
			return getattr(self.vocabulary, name)
		else:
			raise AttributeError(f"'{type(self.vocabulary).__name__}' object has no attribute '{name}'")

	def __deepcopy__(self, memo):
		cls = self.__class__
		result = cls.__new__(cls)
		memo[id(self)] = result
		
		result.huggingface_name = copy.deepcopy(self.huggingface_name, memo)
		result.name = copy.deepcopy(self.name, memo)
		result.vocab_size = self.vocab_size
		result.bos_token_id = self.bos_token_id
		result.eos_token_id = self.eos_token_id
		result.pad_token_id = self.pad_token_id
		result.sep_token_id = self.sep_token_id
		result.add_bos_token = self.add_bos_token
		result.add_eos_token = self.add_eos_token

		result.tokenizer = self.tokenizer
		return result
		
	def tokenize(self, input_str):
		return self.tokenizer.tokenize(input_str)

	def from_pretrained(self, *args, **kwargs):
		return LlamaTokenizerFast(*args, **kwargs)
	
	def encode(self, input_str):
		if not len(input_str):
			return np.int32([])
		
		encoded = self.tokenizer.encode(input_str)

		if isinstance(input_str, list):
			if self.add_bos_token:
				for e in encoded:
					e.insert(0, self.bos_token_id)
			if self.add_eos_token:
				for e in encoded:
					e.append(self.eos_token_id)
			return [(e) for e in encoded]
		else:
			if self.add_bos_token:
				encoded.insert(0, self.bos_token_id)
			if self.add_eos_token:
				encoded.append(self.eos_token_id)
			return (encoded)
		
	def __call__(self, input_str):
		return self.encode(input_str)
	
	
	def _decode(self, input_ids):
		if input_ids is None:
			return ""
		try:
			if len(input_ids) == 0:
				return ""
		except:
			pass
		if isinstance(input_ids, np.ndarray):
			input_ids = input_ids.tolist()
		if isinstance(input_ids, int):
			input_ids = [input_ids,]
		if isinstance(input_ids[0], list):
			return [self.decode(e) for e in input_ids]
		
		our_str = self.tokenizer.decode(input_ids)

		for special_matches_idx, (idx, query, value) in enumerate(self.special_matches_decoding):
			# our_str = our_str.replace(value, query)
			our_str = our_str.replace(query, value)

		return our_str

	
	def decode(self, input_ids):
		try:
			return self._decode(input_ids)
		except:
			try:
				return self._decode(np.int32(input_ids))
			except:
				return [self._decode(np.int32(e)) for e in input_ids]
	
	
	def batch_decode(self, input_ids):
		if isinstance(input_ids, np.ndarray):
			input_ids = input_ids.tolist()
		return [self.decode(e) for e in input_ids]
		



if __name__ == "__main__":
	tokenizer = Tokenizer()
	print(f"tokenizer: {tokenizer}")

	text = "What color is the sky?"
	encoded = tokenizer.encode(text)
	print(f"encoded: {encoded}")


